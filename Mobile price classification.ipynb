{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":15520,"sourceType":"datasetVersion","datasetId":11167}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-17T06:33:11.799627Z","iopub.execute_input":"2024-08-17T06:33:11.800135Z","iopub.status.idle":"2024-08-17T06:33:12.330386Z","shell.execute_reply.started":"2024-08-17T06:33:11.800098Z","shell.execute_reply":"2024-08-17T06:33:12.328795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pathlib import Path\nfile_path = Path(\"/kaggle/input/mobile-price-classification\")\ntrain_df = pd.read_csv(file_path/\"train.csv\")\ntest_df = pd.read_csv(file_path/\"test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.334179Z","iopub.execute_input":"2024-08-17T06:33:12.335106Z","iopub.status.idle":"2024-08-17T06:33:12.388684Z","shell.execute_reply.started":"2024-08-17T06:33:12.335049Z","shell.execute_reply":"2024-08-17T06:33:12.387365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Problem statement\nThis is a mobile-price classification problem that dataset is provided in Kaggle.  \nWith 20 features in the dataset, I have to classify each records into 4 categories (0~3).  \n\nThe target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).","metadata":{}},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Dataset\nTrain dataset has 2000 records with 20 features.  \nThere is no null values in each columns.  \n\n- Binary features: blue, dual_sim, four_g, three_g, touch_screen, wifi\n- Continuous features: others except above.\n\nTest dataset has 1000 records with same features.  \n","metadata":{}},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.390686Z","iopub.execute_input":"2024-08-17T06:33:12.391135Z","iopub.status.idle":"2024-08-17T06:33:12.488966Z","shell.execute_reply.started":"2024-08-17T06:33:12.391095Z","shell.execute_reply":"2024-08-17T06:33:12.487981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.490190Z","iopub.execute_input":"2024-08-17T06:33:12.490516Z","iopub.status.idle":"2024-08-17T06:33:12.517214Z","shell.execute_reply.started":"2024-08-17T06:33:12.490484Z","shell.execute_reply":"2024-08-17T06:33:12.515982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.520071Z","iopub.execute_input":"2024-08-17T06:33:12.520426Z","iopub.status.idle":"2024-08-17T06:33:12.528103Z","shell.execute_reply.started":"2024-08-17T06:33:12.520396Z","shell.execute_reply":"2024-08-17T06:33:12.526459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['price_range'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.529970Z","iopub.execute_input":"2024-08-17T06:33:12.530342Z","iopub.status.idle":"2024-08-17T06:33:12.548106Z","shell.execute_reply.started":"2024-08-17T06:33:12.530301Z","shell.execute_reply":"2024-08-17T06:33:12.546904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:33:12.549774Z","iopub.execute_input":"2024-08-17T06:33:12.550464Z","iopub.status.idle":"2024-08-17T06:33:12.559402Z","shell.execute_reply.started":"2024-08-17T06:33:12.550424Z","shell.execute_reply":"2024-08-17T06:33:12.557991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\ncontinuous_df = train_df[[c for c in train_df.columns if c not in ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']]]\nwith pd.option_context('mode.use_inf_as_na', True):\n    sns.pairplot(continuous_df, hue='price_range', diag_kind='kde', markers=[\"o\", \"s\", \"D\", \"^\"])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:45:42.133286Z","iopub.execute_input":"2024-08-17T06:45:42.133802Z","iopub.status.idle":"2024-08-17T06:48:25.640544Z","shell.execute_reply.started":"2024-08-17T06:45:42.133764Z","shell.execute_reply":"2024-08-17T06:48:25.639067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation matrix\nThe feature that is related to price range most is ram.  \nThe other features shows very low correlation with price range.  \nTherefore, continuous variables might not be helpful to classify price range as they are.  \n(transformation might be needed such as log, exp)  ","metadata":{}},{"cell_type":"code","source":"corr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:51:59.495343Z","iopub.execute_input":"2024-08-17T06:51:59.495751Z","iopub.status.idle":"2024-08-17T06:51:59.670957Z","shell.execute_reply.started":"2024-08-17T06:51:59.495720Z","shell.execute_reply":"2024-08-17T06:51:59.669797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Transformation on continous features\nI applied two transformation to the continuous features: log and exp.  \nBut it seems there is not remarkable improvement.  \n","metadata":{}},{"cell_type":"markdown","source":"### Log","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(file_path/\"train.csv\")\ntest_df = pd.read_csv(file_path/\"test.csv\")\nbinary_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\ncontinuous_features = [c for c in train_df.columns if c not in binary_features + ['price_range']]\nlog_features = [f'{c}_log' for c in continuous_features]\ntrain_df[log_features] = train_df[continuous_features].apply(lambda x: np.log1p(x))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:05.342678Z","iopub.execute_input":"2024-08-17T06:52:05.343297Z","iopub.status.idle":"2024-08-17T06:52:05.385512Z","shell.execute_reply.started":"2024-08-17T06:52:05.343262Z","shell.execute_reply":"2024-08-17T06:52:05.384232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[continuous_features].head()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:06.804053Z","iopub.execute_input":"2024-08-17T06:52:06.804478Z","iopub.status.idle":"2024-08-17T06:52:06.826014Z","shell.execute_reply.started":"2024-08-17T06:52:06.804443Z","shell.execute_reply":"2024-08-17T06:52:06.824564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[log_features].head()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:08.400963Z","iopub.execute_input":"2024-08-17T06:52:08.401343Z","iopub.status.idle":"2024-08-17T06:52:08.425219Z","shell.execute_reply.started":"2024-08-17T06:52:08.401313Z","shell.execute_reply":"2024-08-17T06:52:08.423891Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"log_corr = train_df[log_features+['price_range']].corr()\nlog_corr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:09.893962Z","iopub.execute_input":"2024-08-17T06:52:09.894460Z","iopub.status.idle":"2024-08-17T06:52:09.957012Z","shell.execute_reply.started":"2024-08-17T06:52:09.894422Z","shell.execute_reply":"2024-08-17T06:52:09.955564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### exp","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(file_path/\"train.csv\")\ntest_df = pd.read_csv(file_path/\"test.csv\")\nbinary_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\ncontinuous_features = [c for c in train_df.columns if c not in binary_features + ['price_range']]\nexp_features = [f'{c}_exp' for c in continuous_features]\ntrain_df[exp_features] = train_df[continuous_features].apply(lambda x: np.exp(x))","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:12.594787Z","iopub.execute_input":"2024-08-17T06:52:12.595168Z","iopub.status.idle":"2024-08-17T06:52:12.627670Z","shell.execute_reply.started":"2024-08-17T06:52:12.595140Z","shell.execute_reply":"2024-08-17T06:52:12.626407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df[exp_features].head()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:14.747287Z","iopub.execute_input":"2024-08-17T06:52:14.747730Z","iopub.status.idle":"2024-08-17T06:52:14.774462Z","shell.execute_reply.started":"2024-08-17T06:52:14.747697Z","shell.execute_reply":"2024-08-17T06:52:14.772993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp_corr = train_df[exp_features+['price_range']].corr()\nexp_corr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:18.166317Z","iopub.execute_input":"2024-08-17T06:52:18.166731Z","iopub.status.idle":"2024-08-17T06:52:18.221215Z","shell.execute_reply.started":"2024-08-17T06:52:18.166700Z","shell.execute_reply":"2024-08-17T06:52:18.219661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature creation\nSome features like pixcel height, pixcel width, screen height, and screen width can be multiplied to create new features.  \nThew new features such pixcel dimesion and screen dimension shows a bit higher correlation than each original features.  \n\n","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(file_path/\"train.csv\")\ntest_df = pd.read_csv(file_path/\"test.csv\")\nbinary_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\ncontinuous_features = [c for c in train_df.columns if c not in binary_features + ['price_range']]\ntrain_df['px_dimension'] = train_df['px_height'] * train_df['px_width']\ntrain_df['sc_dimension'] = train_df['sc_h'] * train_df['sc_w']\ncorr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:20.868174Z","iopub.execute_input":"2024-08-17T06:52:20.868610Z","iopub.status.idle":"2024-08-17T06:52:20.968604Z","shell.execute_reply.started":"2024-08-17T06:52:20.868578Z","shell.execute_reply":"2024-08-17T06:52:20.967418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bar plot\nDrawing bar plot for each binary features shows how records are distributed by features.  \n- **Bluetuth**: class 3 mobile is likely to have bluetuth option more than other classes.\n- **Dual sim**: class 3 mobile is likely to have dual sim option more than other classes.\n- **Four G**: class 0,1,3 mobile is likely to have 4G option more than class 2.\n- **Three G**: does not show proportion difference between classes.\n- **Touch screen**: class 0,1 mobile is likely to have tourch screen option more than class 2,3\n- **Wifi**: class 3 mobile is likely to have wifi option more than other classes.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:24.630752Z","iopub.execute_input":"2024-08-17T06:52:24.631159Z","iopub.status.idle":"2024-08-17T06:52:24.636852Z","shell.execute_reply.started":"2024-08-17T06:52:24.631130Z","shell.execute_reply":"2024-08-17T06:52:24.635452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"binary_features = ['blue', 'dual_sim', 'four_g', 'three_g', 'touch_screen', 'wifi']\nfig, ((ax1,ax2,ax3),(ax4,ax5,ax6)) = plt.subplots(2,3, figsize=(10,5))\naxes = [ax1, ax2, ax3, ax4, ax5, ax6]\nfor idx, (bf,ax) in enumerate(zip(binary_features, axes)):\n    counts = train_df.groupby(['price_range', bf]).size().unstack(fill_value=0)\n    counts = counts / 500 # normalize each counts as dividing by 500 (500 records for each classes)\n    counts.plot(kind='bar', ax=ax)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:25.638321Z","iopub.execute_input":"2024-08-17T06:52:25.639732Z","iopub.status.idle":"2024-08-17T06:52:27.255676Z","shell.execute_reply.started":"2024-08-17T06:52:25.639686Z","shell.execute_reply":"2024-08-17T06:52:27.254376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data split\nData split is crucial to train model as avoiding overfitting.  \nThe purpose of deep learning model is generalizing patterns.  \nloss from validation dataset will show how much model generalizes.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:31.369993Z","iopub.execute_input":"2024-08-17T06:52:31.370761Z","iopub.status.idle":"2024-08-17T06:52:31.561587Z","shell.execute_reply.started":"2024-08-17T06:52:31.370726Z","shell.execute_reply":"2024-08-17T06:52:31.560420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:32.633822Z","iopub.execute_input":"2024-08-17T06:52:32.634662Z","iopub.status.idle":"2024-08-17T06:52:32.641643Z","shell.execute_reply.started":"2024-08-17T06:52:32.634629Z","shell.execute_reply":"2024-08-17T06:52:32.640316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA\nPCA reduces dimension of dataset as get principal components.  \nSince the most continuous features are uncorrelated to target, it might be better to try dimension reduction. I prepared PCA dataset as anohter option.  \n\nPCA should include only continuous numerical features, so exclude binary features from PCA and add them as they are in final dataset.  ","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:34.974845Z","iopub.execute_input":"2024-08-17T06:52:34.975241Z","iopub.status.idle":"2024-08-17T06:52:35.111258Z","shell.execute_reply.started":"2024-08-17T06:52:34.975212Z","shell.execute_reply":"2024-08-17T06:52:35.110149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.columns","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:36.929556Z","iopub.execute_input":"2024-08-17T06:52:36.929939Z","iopub.status.idle":"2024-08-17T06:52:36.938861Z","shell.execute_reply.started":"2024-08-17T06:52:36.929911Z","shell.execute_reply":"2024-08-17T06:52:36.937307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(continuous_features)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:39.272611Z","iopub.execute_input":"2024-08-17T06:52:39.273092Z","iopub.status.idle":"2024-08-17T06:52:39.281476Z","shell.execute_reply.started":"2024-08-17T06:52:39.273058Z","shell.execute_reply":"2024-08-17T06:52:39.280095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\npca = PCA()\nx = train_df[continuous_features]\nscaled_data = scaler.fit_transform(x)\nprintcipalComponents = pca.fit_transform(scaled_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:40.303459Z","iopub.execute_input":"2024-08-17T06:52:40.303924Z","iopub.status.idle":"2024-08-17T06:52:40.331046Z","shell.execute_reply.started":"2024-08-17T06:52:40.303891Z","shell.execute_reply":"2024-08-17T06:52:40.329347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below plot shows how much variances are explained by Principla components.  \nI'll choose the point that slope starts to flatten since the flatten slope means adding more component is less efficient from that point.  \nIn this case, 11 components seems to be appropriate.  ","metadata":{}},{"cell_type":"code","source":"explained_variance = pca.explained_variance_ratio_\ncumulative_explained_variance = explained_variance.cumsum()\nplt.figure(figsize=(8, 6))\nplt.plot(range(1, len(explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\nplt.title('Explained Variance by Principal Components')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.axhline(y=0.9, color='r', linestyle='-')\nplt.text(0.5, 0.85, '90% variance threshold', color = 'red', fontsize=12)\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:42.669061Z","iopub.execute_input":"2024-08-17T06:52:42.669542Z","iopub.status.idle":"2024-08-17T06:52:43.057926Z","shell.execute_reply.started":"2024-08-17T06:52:42.669490Z","shell.execute_reply":"2024-08-17T06:52:43.056535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()\npca = PCA(n_components=11)\nx = train_df[[c for c in train_df.columns if c not in ['price_range']]]\nscaled_data = scaler.fit_transform(x)\nprintcipalComponents = pca.fit_transform(scaled_data)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:45.895761Z","iopub.execute_input":"2024-08-17T06:52:45.896219Z","iopub.status.idle":"2024-08-17T06:52:45.950726Z","shell.execute_reply.started":"2024-08-17T06:52:45.896185Z","shell.execute_reply":"2024-08-17T06:52:45.948795Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_df = pd.DataFrame(printcipalComponents, columns=[f'PCA{i+1}' for i in range(11)])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:47.685323Z","iopub.execute_input":"2024-08-17T06:52:47.685753Z","iopub.status.idle":"2024-08-17T06:52:47.692891Z","shell.execute_reply.started":"2024-08-17T06:52:47.685722Z","shell.execute_reply":"2024-08-17T06:52:47.691281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:49.117785Z","iopub.execute_input":"2024-08-17T06:52:49.118696Z","iopub.status.idle":"2024-08-17T06:52:49.137233Z","shell.execute_reply.started":"2024-08-17T06:52:49.118663Z","shell.execute_reply":"2024-08-17T06:52:49.136034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I trained models as changing datasets: PCA, original. ","metadata":{}},{"cell_type":"code","source":"X = train_df[binary_features + ['ram', 'px_width','px_height','sc_h', 'sc_w', 'battery_power']]\n# X = pd.concat([pca_df, train_df[binary_features]], axis=1)\ny = train_df['price_range']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:51.220956Z","iopub.execute_input":"2024-08-17T06:52:51.221357Z","iopub.status.idle":"2024-08-17T06:52:51.237147Z","shell.execute_reply.started":"2024-08-17T06:52:51.221327Z","shell.execute_reply":"2024-08-17T06:52:51.235588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:53.427094Z","iopub.execute_input":"2024-08-17T06:52:53.427625Z","iopub.status.idle":"2024-08-17T06:52:53.436304Z","shell.execute_reply.started":"2024-08-17T06:52:53.427579Z","shell.execute_reply":"2024-08-17T06:52:53.434920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([X_train, y_train], axis=1)\nval = pd.concat([X_val, y_val], axis=1)\ntest = pd.concat([X_test, y_test], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:55.117336Z","iopub.execute_input":"2024-08-17T06:52:55.118676Z","iopub.status.idle":"2024-08-17T06:52:55.129503Z","shell.execute_reply.started":"2024-08-17T06:52:55.118615Z","shell.execute_reply":"2024-08-17T06:52:55.128132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Build Pytorch model\nI will build deep learning model using pytorch tabular.\nPytorch tabular makes model building procedure simpler.  ","metadata":{}},{"cell_type":"code","source":"!pip install pytorch_tabular -q","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:52:58.038673Z","iopub.execute_input":"2024-08-17T06:52:58.039086Z","iopub.status.idle":"2024-08-17T06:53:29.918515Z","shell.execute_reply.started":"2024-08-17T06:52:58.039057Z","shell.execute_reply":"2024-08-17T06:53:29.917170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"pytorch tabular package works with scikit-learn 1.2.2 version.","metadata":{}},{"cell_type":"code","source":"!pip install scikit-learn==1.2.2 -q","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:29.921044Z","iopub.execute_input":"2024-08-17T06:53:29.921406Z","iopub.status.idle":"2024-08-17T06:53:48.647212Z","shell.execute_reply.started":"2024-08-17T06:53:29.921374Z","shell.execute_reply":"2024-08-17T06:53:48.645843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabular import TabularModel\nfrom pytorch_tabular.models import CategoryEmbeddingModelConfig\nfrom pytorch_tabular.config import (\n    DataConfig,\n    OptimizerConfig,\n    TrainerConfig,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:48.649282Z","iopub.execute_input":"2024-08-17T06:53:48.649723Z","iopub.status.idle":"2024-08-17T06:53:58.718814Z","shell.execute_reply.started":"2024-08-17T06:53:48.649684Z","shell.execute_reply":"2024-08-17T06:53:58.717206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:58.721550Z","iopub.execute_input":"2024-08-17T06:53:58.721966Z","iopub.status.idle":"2024-08-17T06:53:58.743877Z","shell.execute_reply.started":"2024-08-17T06:53:58.721935Z","shell.execute_reply":"2024-08-17T06:53:58.742314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:58.745506Z","iopub.execute_input":"2024-08-17T06:53:58.746015Z","iopub.status.idle":"2024-08-17T06:53:58.752017Z","shell.execute_reply.started":"2024-08-17T06:53:58.745973Z","shell.execute_reply":"2024-08-17T06:53:58.750809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameter tuning\nI tested combination of layers, dropout, and optimizers.  \nAnd the best combination shows **94%** of accuracy.  ","metadata":{}},{"cell_type":"code","source":"target = ['price_range']\ncontinuous_cols = ['ram', 'px_width','px_height','sc_h','sc_w','battery_power']\n# continuous_cols = [f'PCA{i+1}' for i in range(11)]\ndata_config = DataConfig(\n    target=target,\n    continuous_cols=continuous_cols,\n    categorical_cols=binary_features\n)\n\ntrainer_config = TrainerConfig(\n    auto_lr_find=True,\n    batch_size=64,\n    max_epochs=200,\n    early_stopping=\"valid_loss\",  # Monitor valid_loss for early stopping\n    early_stopping_mode=\"min\",  # Set the mode as min because for val_loss, lower is better\n    early_stopping_patience=5,  # No. of epochs of degradation training will wait before terminating\n    checkpoints=\"valid_loss\",  # Save best checkpoint monitoring val_loss\n    load_best=True,  # After training, load the best checkpoint\n)\noptimizer_config = OptimizerConfig()\n\nmodel_config = CategoryEmbeddingModelConfig(\n    task='classification',\n    layers='1024-1024-512-256-128',\n    activation='LeakyReLU',\n    learning_rate=1e-6,\n)\n\nsearch_space = {\n    \"model_config__layers\": [\"1024-512-512\", \"1024-512-256\", \"1024-512-128\"],\n    \"model_config.head_config__dropout\": [0.1, 0.2, 0.3],\n    \"optimizer_config__optimizer\": [\"RAdam\", \"AdamW\"],\n}","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:58.753440Z","iopub.execute_input":"2024-08-17T06:53:58.753892Z","iopub.status.idle":"2024-08-17T06:53:58.765316Z","shell.execute_reply.started":"2024-08-17T06:53:58.753860Z","shell.execute_reply":"2024-08-17T06:53:58.764037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_tabular.tabular_model_tuner import TabularModelTuner","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:53:58.766796Z","iopub.execute_input":"2024-08-17T06:53:58.767167Z","iopub.status.idle":"2024-08-17T06:53:58.781112Z","shell.execute_reply.started":"2024-08-17T06:53:58.767139Z","shell.execute_reply":"2024-08-17T06:53:58.779980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tuner = TabularModelTuner(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config\n)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    result = tuner.tune(\n        train=train,\n        validation=test,\n        search_space=search_space,\n        strategy=\"grid_search\",\n        # cv=5, # Uncomment this to do a 5 fold cross validation\n        metric=\"accuracy\",\n        mode=\"max\",\n        progress_bar=True,\n        verbose=False # Make True if you want to log metrics and params each iteration\n    )","metadata":{"execution":{"iopub.status.busy":"2024-08-17T06:59:01.227899Z","iopub.execute_input":"2024-08-17T06:59:01.228477Z","iopub.status.idle":"2024-08-17T07:02:06.839914Z","shell.execute_reply.started":"2024-08-17T06:59:01.228439Z","shell.execute_reply":"2024-08-17T07:02:06.838756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result.trials_df.sort_values(by='accuracy')","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:02:19.168319Z","iopub.execute_input":"2024-08-17T07:02:19.168813Z","iopub.status.idle":"2024-08-17T07:02:19.193490Z","shell.execute_reply.started":"2024-08-17T07:02:19.168777Z","shell.execute_reply":"2024-08-17T07:02:19.191999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Best Score: \", result.best_score)\nprint(result.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:11:23.320502Z","iopub.execute_input":"2024-08-17T07:11:23.321134Z","iopub.status.idle":"2024-08-17T07:11:23.328602Z","shell.execute_reply.started":"2024-08-17T07:11:23.321099Z","shell.execute_reply":"2024-08-17T07:11:23.327207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabuler_model_best = result.best_model","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:11:58.386463Z","iopub.execute_input":"2024-08-17T07:11:58.386983Z","iopub.status.idle":"2024-08-17T07:11:58.393454Z","shell.execute_reply.started":"2024-08-17T07:11:58.386947Z","shell.execute_reply":"2024-08-17T07:11:58.391985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabuler_model","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:11:26.342511Z","iopub.execute_input":"2024-08-17T07:11:26.342977Z","iopub.status.idle":"2024-08-17T07:11:26.351148Z","shell.execute_reply.started":"2024-08-17T07:11:26.342945Z","shell.execute_reply":"2024-08-17T07:11:26.349713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tabular_model = TabularModel(\n    data_config=data_config,\n    model_config=model_config,\n    optimizer_config=optimizer_config,\n    trainer_config=trainer_config,\n)\ntabular_model.fit(train=train, validation=val)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:11:27.776244Z","iopub.execute_input":"2024-08-17T07:11:27.776759Z","iopub.status.idle":"2024-08-17T07:11:34.931819Z","shell.execute_reply.started":"2024-08-17T07:11:27.776724Z","shell.execute_reply":"2024-08-17T07:11:34.930610Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = tabuler_model_best.evaluate(test)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:07.346804Z","iopub.execute_input":"2024-08-17T07:12:07.347263Z","iopub.status.idle":"2024-08-17T07:12:07.564213Z","shell.execute_reply.started":"2024-08-17T07:12:07.347230Z","shell.execute_reply":"2024-08-17T07:12:07.562946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df = tabuler_model_best.predict(test)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:16.133639Z","iopub.execute_input":"2024-08-17T07:12:16.134137Z","iopub.status.idle":"2024-08-17T07:12:16.266487Z","shell.execute_reply.started":"2024-08-17T07:12:16.134105Z","shell.execute_reply":"2024-08-17T07:12:16.265204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_df","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:18.361247Z","iopub.execute_input":"2024-08-17T07:12:18.361744Z","iopub.status.idle":"2024-08-17T07:12:18.380645Z","shell.execute_reply.started":"2024-08-17T07:12:18.361710Z","shell.execute_reply":"2024-08-17T07:12:18.379132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:21.130943Z","iopub.execute_input":"2024-08-17T07:12:21.131400Z","iopub.status.idle":"2024-08-17T07:12:21.139974Z","shell.execute_reply.started":"2024-08-17T07:12:21.131368Z","shell.execute_reply":"2024-08-17T07:12:21.138605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"report = classification_report(y_test, pred_df['prediction'])","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:22.691944Z","iopub.execute_input":"2024-08-17T07:12:22.692425Z","iopub.status.idle":"2024-08-17T07:12:22.711491Z","shell.execute_reply.started":"2024-08-17T07:12:22.692390Z","shell.execute_reply":"2024-08-17T07:12:22.710034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Classification report of the best model.  \nF1 score for each classes is more than 0.90.  \nSo the overall F1 score becomes 0.95\nRecall score is more important in most cases therefore it is remarkable that each recall score is more than 0.90  ","metadata":{}},{"cell_type":"code","source":"print(report)","metadata":{"execution":{"iopub.status.busy":"2024-08-17T07:12:25.155085Z","iopub.execute_input":"2024-08-17T07:12:25.155538Z","iopub.status.idle":"2024-08-17T07:12:25.162484Z","shell.execute_reply.started":"2024-08-17T07:12:25.155491Z","shell.execute_reply":"2024-08-17T07:12:25.160965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# History summary and conclusion\n\n|Features                                                                      |Layer            |Accuracy |F1 score |\n|------------------------------------------------------------------------------|-----------------|--------:|--------:|\n|binary columns + ram, battery_power, px_dimension                             |1024-512         |0.92     |0.92     |\n|binary columns + ram, battery_power, px_dimension                             |512-256-128-64   |0.92     |0.92     |\n|binary columns + ram, px_width, px_height, sc_h, sc_w, battery_power|512-256-128-64|0.91|0.91|\n|PCA4                                                                |512-256       |0.92|0.92|\n|binary columns + ram, px_width, px_height, sc_h, sc_w, battery_power|1024-512-128|0.95|0.95     |\n\nWith pytorch tabular packages, I could test several architectures and compare each models.  \nSince my dataset has only 2000 records and they are balanced It is hard to drop some records.  \nAdditionally in correlation matrix and pairwise plot, I couldn't find outliers.  \nMost features are uncorrelated each other, so it was not easy find patterns in graph.  \n\nAccording to the classification report, If I could find or create feature that is helpful to identify pattern of class 2,  \nModel could imporve more.  \n","metadata":{}}]}